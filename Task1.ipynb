{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WYMfvCNPwpm"
   },
   "source": [
    "# Project A: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vA8ppgB2P0aJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "builder = tfds.builder('mnist')\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 12\n",
    "NUM_CLASSES = 10  # 10 total classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2EFLQROP2R7"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ynByMG_UP4A4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\ayman\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba33f873f7e43f9aff10678c0ecbc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d58a793ef040d79e5410584ee7375f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771e2ad16af841ea94dda3d7048176a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f0184aa8f04a9980b2cbb8eae2b420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23766ba638bf40a2ad08b1089fcc56fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681966cff2b424387849fdfd9bde384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ayman\\tensorflow_datasets\\mnist\\3.0.1.incompleteJ79E6Q\\mnist-train.tfrecord*...:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2e925d8a2546b59f01b7304b32e22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f709a0552d493da953261c519dd66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ayman\\tensorflow_datasets\\mnist\\3.0.1.incompleteJ79E6Q\\mnist-test.tfrecord*...:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\ayman\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Trying to access `splits['train']` but `splits` is empty. This likely indicate the dataset has not been generated yet.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ayman\\OneDrive - University of Toronto\\PhD courses\\ECE1512H  Digital Image Processing\\Project A\\Project_A_Supp\\Project_A_Supp\\Task1.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/OneDrive%20-%20University%20of%20Toronto/PhD%20courses/ECE1512H%20%20Digital%20Image%20Processing/Project%20A/Project_A_Supp/Project_A_Supp/Task1.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m mnist_train \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmnist\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, shuffle_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mcache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/OneDrive%20-%20University%20of%20Toronto/PhD%20courses/ECE1512H%20%20Digital%20Image%20Processing/Project%20A/Project_A_Supp/Project_A_Supp/Task1.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m mnist_train \u001b[39m=\u001b[39m mnist_train\u001b[39m.\u001b[39mmap(preprocess)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ayman/OneDrive%20-%20University%20of%20Toronto/PhD%20courses/ECE1512H%20%20Digital%20Image%20Processing/Project%20A/Project_A_Supp/Project_A_Supp/Task1.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m mnist_train \u001b[39m=\u001b[39m mnist_train\u001b[39m.\u001b[39mshuffle(builder\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mnum_examples)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/OneDrive%20-%20University%20of%20Toronto/PhD%20courses/ECE1512H%20%20Digital%20Image%20Processing/Project%20A/Project_A_Supp/Project_A_Supp/Task1.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m mnist_train \u001b[39m=\u001b[39m mnist_train\u001b[39m.\u001b[39mbatch(BATCH_SIZE, drop_remainder\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/OneDrive%20-%20University%20of%20Toronto/PhD%20courses/ECE1512H%20%20Digital%20Image%20Processing/Project%20A/Project_A_Supp/Project_A_Supp/Task1.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m mnist_test \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmnist\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcache()\n",
      "File \u001b[1;32mc:\\Users\\ayman\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:371\u001b[0m, in \u001b[0;36mSplitDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m    370\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrying to access `splits[\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m]` but `splits` is empty. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    373\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mThis likely indicate the dataset has not been generated yet.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    374\u001b[0m   \u001b[39m# 1st case: The key exists: `info.splits['train']`\u001b[39;00m\n\u001b[0;32m    375\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39mstr\u001b[39m(key) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Trying to access `splits['train']` but `splits` is empty. This likely indicate the dataset has not been generated yet.\""
     ]
    }
   ],
   "source": [
    "# Load train and test splits.\n",
    "def preprocess(x):\n",
    "  image = tf.image.convert_image_dtype(x['image'], tf.float32)\n",
    "  subclass_labels = tf.one_hot(x['label'], builder.info.features['label'].num_classes)\n",
    "  return image, subclass_labels\n",
    "\n",
    "\n",
    "mnist_train = tfds.load('mnist', split='train', shuffle_files=False).cache()\n",
    "mnist_train = mnist_train.map(preprocess)\n",
    "mnist_train = mnist_train.shuffle(builder.info.splits['train'].num_examples)\n",
    "mnist_train = mnist_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "mnist_test = tfds.load('mnist', split='test').cache()\n",
    "mnist_test = mnist_test.map(preprocess).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZwfvW5P63q"
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zINgDkA7P7BP"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Build CNN teacher.\n",
    "cnn_model = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for stpe 2\n",
    "\n",
    "\n",
    "\n",
    "# Build fully connected student.\n",
    "fc_model = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "# your code start from here for step 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JWGucyrQGav"
   },
   "source": [
    "# Teacher loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhzBP6ZLQJ57"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_teacher_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  subclass_logits = cnn_model(images, training=True)\n",
    "\n",
    "  # Compute cross-entropy loss for subclasses.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "  cross_entropy_loss_value =\n",
    "\n",
    "\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS8xkuH0QbOS"
   },
   "source": [
    "# Student loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDKia4gPQMIr"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Hyperparameters for distillation (need to be tuned).\n",
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
    "\n",
    "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "  \"\"\"Compute distillation loss.\n",
    "\n",
    "  This function computes cross entropy between softened logits and softened\n",
    "  targets. The resulting loss is scaled by the squared temperature so that\n",
    "  the gradient magnitude remains approximately constant as the temperature is\n",
    "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
    "  a neural network.\"\n",
    "\n",
    "  Args:\n",
    "    teacher_logits: A Tensor of logits provided by the teacher.\n",
    "    student_logits: A Tensor of logits provided by the student, of the same\n",
    "      shape as `teacher_logits`.\n",
    "    temperature: Temperature to use for distillation.\n",
    "\n",
    "  Returns:\n",
    "    A scalar Tensor containing the distillation loss.\n",
    "  \"\"\"\n",
    " # your code start from here for step 3\n",
    "  soft_targets = \n",
    "\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "def compute_student_loss(images, labels):\n",
    "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
    "     and labels.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  student_subclass_logits = fc_model(images, training=True)\n",
    "\n",
    "  # Compute subclass distillation loss between student subclass logits and\n",
    "  # softened teacher subclass targets probabilities.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  teacher_subclass_logits = cnn_model(images, training=False)\n",
    "  distillation_loss_value =\n",
    "\n",
    "  # Compute cross-entropy loss with hard targets.\n",
    "\n",
    "  # your code start from here for step 3\n",
    "\n",
    "  cross_entropy_loss_value = \n",
    "\n",
    "  return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ1uyvurQ3w4"
   },
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtoLbp8uQ4Vl"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_num_correct(model, images, labels):\n",
    "  \"\"\"Compute number of correctly classified images in a batch.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Number of correctly classified images.\n",
    "  \"\"\"\n",
    "  class_logits = model(images, training=False)\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn):\n",
    "  \"\"\"Perform training and evaluation for a given model.\n",
    "\n",
    "  Args:\n",
    "    model: Instance of tf.keras.Model.\n",
    "    compute_loss_fn: A function that computes the training loss given the\n",
    "      images, and labels.\n",
    "  \"\"\"\n",
    "\n",
    "  # your code start from here for step 4\n",
    "  optimizer = \n",
    "\n",
    "  for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Run training.\n",
    "    print('Epoch {}: '.format(epoch), end='')\n",
    "    for images, labels in mnist_train:\n",
    "      with tf.GradientTape() as tape:\n",
    "         # your code start from here for step 4\n",
    "\n",
    "        loss_value = \n",
    "\n",
    "      grads = \n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    num_correct = 0\n",
    "    num_total = builder.info.splits['test'].num_examples\n",
    "    for images, labels in mnist_test:\n",
    "      # your code start from here for step 4\n",
    "      num_correct += \n",
    "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
    "        num_correct / num_total * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQL1lJdaRPT1"
   },
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AGHbyABRPz3"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj1N38fnRTNB"
   },
   "source": [
    "# Test accuracy vs. tempreture curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX4dbazrRWIz"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNrH_1emRbGA"
   },
   "source": [
    "# Train student from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjospsxIRbQ6"
   },
   "outputs": [],
   "source": [
    "# Build fully connected student.\n",
    "fc_model_no_distillation = tf.keras.Sequential()\n",
    "\n",
    "# your code start from here for step 7\n",
    "\n",
    "\n",
    "\n",
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "def compute_plain_cross_entropy_loss(images, labels):\n",
    "  \"\"\"Compute plain loss for given images and labels.\n",
    "\n",
    "  For fair comparison and convenience, this function also performs a\n",
    "  LogSumExp over subclasses, but does not perform subclass distillation.\n",
    "\n",
    "  Args:\n",
    "    images: Tensor representing a batch of images.\n",
    "    labels: Tensor representing a batch of labels.\n",
    "\n",
    "  Returns:\n",
    "    Scalar loss Tensor.\n",
    "  \"\"\"\n",
    "  # your code start from here for step 7\n",
    "\n",
    "  student_subclass_logits = fc_model_no_distillation(images, training=True)\n",
    "  cross_entropy_loss = \n",
    "  \n",
    "  return cross_entropy_loss\n",
    "\n",
    "\n",
    "train_and_evaluate(fc_model_no_distillation, compute_plain_cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq3JTpQ4RuhR"
   },
   "source": [
    "# Comparing the teacher and student model (number of of parameters and FLOPs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4V8GB2yRRuxF"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjwJ5oziRvRn"
   },
   "source": [
    "# Implementing the state-of-the-art KD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q10lybAFRvZt"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dsOmtqdieIC"
   },
   "source": [
    "# (Optional) XAI method to explain models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0IMIFW8ilPO"
   },
   "outputs": [],
   "source": [
    "# your code start from here for step 13\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "680419e0188aaa6743f4a0e8f0e13458aa11f8cd04d9d5e558d5d63e6a49fb4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
